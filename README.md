
# ğŸ›¢ï¸ Commodity Data Platform

A simulated data engineering platform for commodity trading analytics, inspired by a real-world Glencore use case. This project demonstrates end-to-end ingestion, transformation, validation, and API delivery of commodity market data using modern tools like Airflow, Spark, FastAPI, and Great Expectations.

---

## ğŸ“Š Use Case

Designed to support trading analysts by delivering clean, transformed, and validated data from external sources (EIA API, web news scraping). The project includes:

- **Batch Ingestion** from APIs and websites
- **Spark-based Transformation** and optimization
- **Automated Data Validation** using Great Expectations
- **API Layer** with FastAPI
- **Workflow Orchestration** with Apache Airflow
- **CI/CD** using GitHub Actions
- **Containerized Setup** with Docker Compose
- **Storage in Parquet Format**

---

## âš™ï¸ Tech Stack

| Layer            | Technology                      |
|------------------|----------------------------------|
| Orchestration    | Apache Airflow                  |
| Ingestion        | Python, Requests, BeautifulSoup |
| Processing       | Apache Spark (PySpark)          |
| Validation       | Great Expectations              |
| Serving          | FastAPI                         |
| CI/CD            | GitHub Actions                  |
| Infrastructure   | Docker Compose                  |
| Storage Format   | Parquet                         |

---

## ğŸ§± Architecture Diagram

```mermaid
graph TD
    A[External APIs: EIA, News Web] --> B[Airflow DAGs]
    B --> C[Ingestion Scripts]
    C --> D[Spark Transformations]
    D --> E[Processed Data (Parquet)]
    E --> F[FastAPI Service]
    E --> G[Great Expectations Validation]
    F --> H[Consumers: Trading Analysts]


```

---

## ğŸ“‚ Project Structure

```
commodity-data-platform/
â”œâ”€â”€ dags/                      # Airflow DAGs
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/             # Data ingestion (API + Web scraping)
â”‚   â”œâ”€â”€ transformation/        # Spark-based transformation
â”‚   â”œâ”€â”€ validation/            # Great Expectations checks
â”‚   â””â”€â”€ api/                   # FastAPI endpoints
â”œâ”€â”€ tests/                     # Unit tests
â”œâ”€â”€ notebooks/                 # EDA and testing
â”œâ”€â”€ .github/workflows/         # CI/CD pipeline
â”œâ”€â”€ docker-compose.yml         # Local dev environment
â”œâ”€â”€ Dockerfile                 # For API or batch jobs
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. Clone & Install
```bash
git clone https://github.com/yourusername/commodity-data-platform.git
cd commodity-data-platform
pip install -r requirements.txt
```

### 2. Run Docker Environment
```bash
docker-compose up --build
```

- Airflow UI: http://localhost:8080  
- Spark UI: http://localhost:8081  
- FastAPI: http://localhost:8000/commodities/oil

### 3. Trigger the DAG

Visit [Airflow UI](http://localhost:8080), enable and run `commodity_data_pipeline`.

---

## ğŸ§ª Sample API Output

```json
GET /commodities/oil

[
  { "date": "2024-05-01", "price": 78.52 },
  { "date": "2024-04-30", "price": 77.83 }
]
```

---

## âœ… Data Validation with Great Expectations

Great Expectations runs automatically in the DAG to ensure:

- Price is within expected range
- Nulls are not present
- Columns are not missing

Use the expectation suite under `src/validation/`.

---

## ğŸ“š Key Learnings

- Modular, testable, and scalable pipeline design
- Real-time data ingestion and validation
- CI/CD pipeline for reliable deployments
- Hands-on integration of Spark, Airflow, and FastAPI

---

## ğŸ‘¤ Author

**Loic**  
Data Engineer | Asset & Analytics Specialist  
_This project simulates real-world responsibilities in a commodity trading context like Glencore._

---

## ğŸ“ License

MIT License â€“ use freely with attribution.
